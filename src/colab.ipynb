{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, RandomFlip, RandomCrop, GlobalAveragePooling2D,BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (2132033305.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3447/2132033305.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "class LoadData:\n",
    "    \"\"\"\n",
    "    LoadData class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_data(self, height, width):\n",
    "      \"\"\"\n",
    "      \n",
    "      \"\"\"\n",
    "      x = []\n",
    "      y = []\n",
    "\n",
    "      print('Read images')\n",
    "      for class_number in range(10):\n",
    "        print(f'Load folder c{class_number}')\n",
    "        class_number_str = 'c' + str(class_number)\n",
    "        path = os.path.join(self.base_path, 'imgs/data', class_number_str, '*.jpg')\n",
    "        file_paths = glob.glob(path)  # Gets all file names matching given path.\n",
    "        sub_x = []\n",
    "        sub_y = []\n",
    "        for file_path in file_paths:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            sub_x.append(file_name)\n",
    "            temp = np.zeros(10)\n",
    "            temp[class_number] = 1\n",
    "            sub_y.append(temp)\n",
    "        x.append(sub_x)\n",
    "        y.append(sub_y)\n",
    "      return x, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "  \"\"\"\n",
    "  Goes through all images, returns preprocessed tensor.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, base_path):\n",
    "    self.base_path = base_path\n",
    "    self.kernel = np.array([[-1, -1, -1],\n",
    "                  [-1, 8,-1],\n",
    "                  [-1, -1, -1]])\n",
    "  \n",
    "  def get_colour_type(self, img_path):\n",
    "    image = cv2.imread(img_path)\n",
    "    if len(image.shape) == 3: return 3\n",
    "    else: return 1\n",
    "\n",
    "  def preprocess_image(self, img_path, height, width):\n",
    "    \"\"\"\n",
    "    Function takes the path to the image and applys the preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    color_type = self.get_colour_type(img_path)\n",
    "\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        img_gray = cv2.threshold(img,0,255,cv2.THRESH_TRUNC+cv2.THRESH_OTSU) \n",
    "        image_sharp = cv2.filter2D(src=img, ddepth=-1, kernel=self.kernel)\n",
    "\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(img_path)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_gray = cv2.threshold(img_gray,0,255,cv2.THRESH_TRUNC+cv2.THRESH_OTSU)\n",
    "        image_sharp = cv2.filter2D(src=img, ddepth=-1, kernel=self.kernel)\n",
    "        image_sharp = cv2.cvtColor(image_sharp, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    combined = cv2.add(image_sharp, img_gray[1])\n",
    "    dst = cv2.resize(combined, (width, height))\n",
    "    img = cv2.cvtColor(dst, cv2.COLOR_GRAY2BGR)\n",
    "    return img\n",
    "\n",
    "  def split_data(self, x, y):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    split_points = self.percent_indexes(x)\n",
    "    for class_num, (xi, yi) in enumerate(zip(x, y)):\n",
    "      \n",
    "      for image_number, (image_path, out) in enumerate(zip(xi, yi)):\n",
    "        if image_number < split_points[class_num]:\n",
    "          \n",
    "          \n",
    "          x_test.append(image)\n",
    "          y_test.append(out)\n",
    "          \n",
    "        else:\n",
    "          \n",
    "          x_train.append(image)\n",
    "          y_train.append(out)\n",
    "    return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)\n",
    "    \n",
    "\n",
    "  def percent_indexes(self, x):\n",
    "    split_points = []\n",
    "    for xi in x:\n",
    "      number_of_images = len(xi)\n",
    "      split_point = int(number_of_images*0.2)\n",
    "      split_points.append(split_point)\n",
    "    return split_points\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Augmentation Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomErasing(layers.Layer):\n",
    "    \"\"\"\n",
    "    Class that performs Random Erasing in Random Erasing Data Augmentation by Zhong et al. \n",
    "    -------------------------------------------------------------------------------------\n",
    "    probability: The probability that the operation will be performed.\n",
    "    sl: min erasing area\n",
    "    sh: max erasing area\n",
    "    r1: min aspect ratio\n",
    "    mean: erasing value\n",
    "    ------\n",
    "    Source: https://github.com/zhunzhong07/Random-Erasing/blob/master/transforms.py\n",
    "    \"\"\"\n",
    "    def __init__(self, probability, sl, sh, r1, mean, **kwargs):\n",
    "        self.probability = probability\n",
    "        self.sl = sl\n",
    "        self.sh = sh\n",
    "        self.r1 = r1\n",
    "        self.mean = mean\n",
    "        super().__init__(**kwargs)        \n",
    "    \n",
    "    def __call__(self, image):\n",
    "        return random_erasing_image(image, self.probability, self.sl, self.sh, self.r1, self.mean)\n",
    "\n",
    "\n",
    "def random_erasing_image(image, probability=0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "    \"\"\"\n",
    "    Function performs random erasing\n",
    "    \"\"\"\n",
    "    if np.random.uniform(0, 1) > probability:\n",
    "        return image\n",
    "    area = image.shape[0] * image.shape[1]\n",
    "    for _ in range(100):\n",
    "        target_area = np.random.uniform(sl, sh) * area\n",
    "        aspect_ratio = np.random.uniform(r1, 1/r1)\n",
    "\n",
    "        h = int(round(np.sqrt(target_area * aspect_ratio)))\n",
    "        w = int(round(np.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "        if w < image.shape[1] and h < image.shape[0]:\n",
    "            x1 = np.random.randint(0, image.shape[0] - h)\n",
    "            y1 = np.random.randint(0, image.shape[1] - w)\n",
    "            if image.shape[2] == 3:\n",
    "                image[x1:x1+h, y1:y1+w, 0] = mean[0]\n",
    "                image[x1:x1+h, y1:y1+w, 1] = mean[1]\n",
    "                image[x1:x1+h, y1:y1+w, 2] = mean[2]\n",
    "            else:\n",
    "                image[x1:x1+h, y1:y1+w, 0] = mean[0]\n",
    "            return image\n",
    "    return image\n",
    "        \n",
    "\n",
    "def random_erasing(image, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n",
    "    \"\"\"\n",
    "    lamdba layer.\n",
    "    \"\"\"\n",
    "    return layers.Lambda(lambda: random_erasing_image(image, probability, sl, sh, r1, mean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.vgg16 = self.build_model()\n",
    "    self.vgg16.summary()\n",
    "\n",
    "  def build_model(self, input_shape=(None,None,3)):\n",
    "    model = Sequential([\n",
    "      Input(shape=input_shape),                               # Block 1\n",
    "      Conv2D(32, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(32, (3, 3), padding='same'),\n",
    "      BatchNormalization(axis = 3),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((2, 2), strides=(2, 2)),                      # Block 2\n",
    "      Conv2D(64, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(64, (3, 3), padding='same'),\n",
    "      BatchNormalization(axis = 3),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((2, 2), strides=(2, 2)),                      # Block 3\n",
    "      Conv2D(128, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(128, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(128, (3, 3), padding='same'),\n",
    "      BatchNormalization(axis = 3),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((2, 2), strides=(2, 2)),                      # Block 4  \n",
    "      Conv2D(256, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(256, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(256, (3, 3), padding='same'),\n",
    "      BatchNormalization(axis = 3),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((2, 2), strides=(2, 2)),                      # Block 5  \n",
    "      Conv2D(256, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(256, (3, 3), padding='same'),\n",
    "      BatchNormalization(),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((1, 1), strides=(1, 1)),\n",
    "      Conv2D(256, (3, 3), padding='same'),\n",
    "      BatchNormalization(axis = 3),\n",
    "      LeakyReLU(),\n",
    "      MaxPool2D((2, 2), strides=(2, 2)),                      # Fully Connected Layers\n",
    "      GlobalAveragePooling2D(),\n",
    "      Dense(512, activation='relu'),\n",
    "      Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "  \n",
    "  def train_model(self, x_train, y_train, x_test, y_test):\n",
    "    self.vgg16.fit(x_train, y_train, epochs=10, verbose=1, batch_size=16)\n",
    "    test_loss, test_acc = self.vgg16.evaluate(x_test, y_test)\n",
    "    print(f'\\nTest lost: {test_loss} -- Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "Read images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n",
      "Load folder c5\n",
      "Load folder c6\n",
      "Load folder c7\n",
      "Load folder c8\n",
      "Load folder c9\n"
     ]
    }
   ],
   "source": [
    "HEIGHT = 96\n",
    "WIDTH = 96\n",
    "\n",
    "load_dotenv()\n",
    "PATH = os.getenv('PATH_TO_DATA')\n",
    "p = PreProcessing(PATH)\n",
    "x, y, driver_ids = p.load_data(HEIGHT, WIDTH)\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_13 (Conv2D)          (None, None, None, 32)    896       \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, None, None, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, None, None, 32)    0         \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, None, None, 32)   0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, None, None, 32)    9248      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, None, None, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, None, None, 32)    0         \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, None, None, 32)   0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, None, None, 64)    18496     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, None, None, 64)   256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, None, None, 64)   0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, None, None, 64)   256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, None, None, 64)   0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, None, None, 128)  512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, None, None, 128)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, None, None, 128)  512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, None, None, 128)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, None, None, 128)  512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, None, None, 128)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, None, None, 256)  1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_20 (LeakyReLU)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, None, None, 256)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, None, None, 256)  1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, None, None, 256)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, None, None, 256)  1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, None, None, 256)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, None, None, 256)  1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, None, None, 256)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, None, None, 256)  1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_24 (LeakyReLU)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, None, None, 256)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, None, None, 256)  1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_25 (LeakyReLU)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, None, None, 256)  0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 256)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,825,322\n",
      "Trainable params: 3,821,098\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "  29/1122 [..............................] - ETA: 5:32 - loss: 2.6258 - accuracy: 0.1336"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28980/4293346775.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28980/3542531512.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nTest lost: {test_loss} -- Test accuracy: {test_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "\n",
    "model.train_model(x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
